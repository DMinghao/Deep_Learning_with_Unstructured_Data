{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "84a4aab85ea450949fc4334acf58296cfde3aae77165464abea9384f31a732b6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Lab 5. TensorFlow and Convolutional Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.keras import TqdmCallback\n",
    "from tqdm import tqdm_notebook\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import plot_model\n",
    "import keras\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import visualkeras as vk\n",
    "from PIL import Image\n",
    "CPU_ONLY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CPU_ONLY: os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(\"CUDA version:\")\n",
    "print(os.popen('nvcc --version').read())\n",
    "with_cuda = tf.test.is_built_with_cuda()\n",
    "print(f\"Can build with CUDA: {with_cuda}\")\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(\"Num GPUs Available: \", len(gpus))\n",
    "for x in gpus: print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n",
    "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(PATH, 'train')\n",
    "validation_dir = os.path.join(PATH, 'validation')\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cats_tr = len(os.listdir(train_cats_dir))\n",
    "num_dogs_tr = len(os.listdir(train_dogs_dir))\n",
    "num_cats_val = len(os.listdir(validation_cats_dir))\n",
    "num_dogs_val = len(os.listdir(validation_dogs_dir))\n",
    "total_train = num_cats_tr + num_dogs_tr\n",
    "total_val = num_cats_val + num_dogs_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total training cat images:', num_cats_tr)\n",
    "print('total training dog images:', num_dogs_tr)\n",
    "print('total validation cat images:', num_cats_val)\n",
    "print('total validation dog images:', num_dogs_val)\n",
    "print(\"--\")\n",
    "print(\"Total training images:\", total_train)\n",
    "print(\"Total validation images:\", total_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_generator = ImageDataGenerator(rescale=1./255)\n",
    "validation_image_generator = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "directory=train_dir, shuffle=True, target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "directory=validation_dir,target_size=(IMG_HEIGHT, IMG_WIDTH),class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_training_images, _ = next(train_data_gen)\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plotImages(sample_training_images[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "model.summary()\n",
    "vk.layered_view(model, spacing=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = TqdmCallback(verbose=1,tqdm_class=tqdm_notebook, leave = True, display = False)\n",
    "pbar.epoch_bar.ncols=0\n",
    "plot = PlotLossesKeras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pbar.display()\n",
    "history = model.fit_generator(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=total_train // batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=0,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=total_val // batch_size,\n",
    "    callbacks=[pbar,plot]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotResult(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss=history.history['loss']\n",
    "    val_loss=history.history['val_loss']\n",
    "    tla = []\n",
    "    vla = []\n",
    "    for i in range(len(acc)): \n",
    "        tla.append(acc[i]*loss[i])\n",
    "        vla.append(val_acc[i]*val_loss[i])\n",
    "    epochs_range = range(epochs)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "plotResult(history)"
   ]
  },
  {
   "source": [
    "## Q1. \n",
    "\n",
    "How would you describe the trend of (1) training accuracy, (2) validation accuracy, (3) training loss, and (4) validation loss?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- Training accuracy: Upward trending \n",
    "- Validation accuracy: Rise quickly then oscillates around 0.73\n",
    "- Training loss: Downward trending \n",
    "- Validation loss: Decreased to around 0.6, follow with a rebound and is then gradually increasing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Q2. \n",
    "\n",
    "Do you observe any issue in the plots, for example, overfitting?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The model is overfitting to the training data, whereas the accuracy for testing data is stagnant. Other than that, the gradually increasing validation loss indicates that the model is starting to \"memorize\" the training data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![](cute_puppy.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=.15,\n",
    "    height_shift_range=.15,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.5\n",
    ")\n",
    "train_data_gen = image_gen_train.flow_from_directory(\n",
    "    batch_size=batch_size,\n",
    "    directory=train_dir, \n",
    "    shuffle=True,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
    "val_data_gen = image_gen_val.flow_from_directory(\n",
    "    batch_size=batch_size,\n",
    "    directory=validation_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    class_mode='binary')"
   ]
  },
  {
   "source": [
    "# Assignment "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelImproved = Sequential([\n",
    "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelImproved.compile(optimizer='adam',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "modelImproved.summary()\n",
    "vk.layered_view(modelImproved, spacing=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = TqdmCallback(verbose=1,tqdm_class=tqdm_notebook, leave = True, display = False)\n",
    "pbar.epoch_bar.ncols=0\n",
    "plot = PlotLossesKeras()\n",
    "epochs = 175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pbar.display()\n",
    "history = modelImproved.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=total_train // batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=0,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=total_val // batch_size,\n",
    "    callbacks=[pbar,plot]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotResult(history)"
   ]
  },
  {
   "source": [
    "## Q3. \n",
    "\n",
    "How would you describe the trend of (1) training accuracy, (2) validation accuracy, (3) training loss, and (4) validation loss?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- Training accuracy: Upward trending \n",
    "- Validation accuracy: Rise along with training accuracy line, oscillates around 0.78\n",
    "- Training loss: Downward trending \n",
    "- Validation loss: Decrease along with training loss line, oscillates around 0.5"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Q4. \n",
    "\n",
    "Do you think the issue before regularization is solved after regularization?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To some degrees, yes, the model is less overfitted after regularization. However, the validation accuracy and validation loss still shows that there could be more improvements."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![](cute_kitpy.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# ARTIFICIAL STUPIDITY"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "accuracy_threshold = 0.98\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_generator = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.5,\n",
    "    zoom_range=0.5,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = train_image_generator.flow_from_directory(\n",
    "    batch_size=batch_size,\n",
    "    directory=train_dir, \n",
    "    shuffle=True, \n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    class_mode='binary')\n",
    "val_data_gen = validation_image_generator.flow_from_directory(\n",
    "    batch_size=batch_size,\n",
    "    directory=validation_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic = Sequential()\n",
    "magic.add(Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n",
    "\n",
    "magic.add(Conv2D(64, (3, 3), padding='same', activation='relu', name='block1_conv1'))\n",
    "magic.add(Conv2D(64, (3, 3), padding='same', activation='relu', name='block1_conv2'))\n",
    "magic.add(MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))\n",
    "magic.add(vk.SpacingDummyLayer(spacing=100))\n",
    "\n",
    "magic.add(Conv2D(128, (3, 3), padding='same', activation='relu', name='block2_conv1'))\n",
    "magic.add(Conv2D(128, (3, 3), padding='same', activation='relu', name='block2_conv2'))\n",
    "magic.add(MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))\n",
    "magic.add(vk.SpacingDummyLayer(spacing=100))\n",
    "\n",
    "magic.add(Conv2D(256, (3, 3), padding='same', activation='relu', name='block3_conv1'))\n",
    "magic.add(Conv2D(256, (3, 3), padding='same', activation='relu', name='block3_conv2'))\n",
    "magic.add(Conv2D(256, (3, 3), padding='same', activation='relu', name='block3_conv3'))\n",
    "magic.add(Conv2D(256, (3, 3), padding='same', activation='relu', name='block3_conv4'))\n",
    "magic.add(MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'))\n",
    "magic.add(vk.SpacingDummyLayer(spacing=100))\n",
    "\n",
    "magic.add(Conv2D(512, (3, 3), padding='same', activation='relu', name='block4_conv1'))\n",
    "magic.add(Conv2D(512, (3, 3), padding='same', activation='relu', name='block4_conv2'))\n",
    "magic.add(Conv2D(512, (3, 3), padding='same', activation='relu', name='block4_conv3'))\n",
    "magic.add(Conv2D(512, (3, 3), padding='same', activation='relu', name='block4_conv4'))\n",
    "magic.add(MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'))\n",
    "magic.add(vk.SpacingDummyLayer(spacing=100))\n",
    "\n",
    "magic.add(Conv2D(512, (3, 3), padding='same', activation='relu', name='block5_conv1'))\n",
    "magic.add(Conv2D(512, (3, 3), padding='same', activation='relu', name='block5_conv2'))\n",
    "magic.add(Conv2D(512, (3, 3), padding='same', activation='relu', name='block5_conv3'))\n",
    "magic.add(Conv2D(512, (3, 3), padding='same', activation='relu', name='block5_conv4'))\n",
    "magic.add(MaxPooling2D((2, 2), padding='same', strides=(2, 2), name='block5_pool'))\n",
    "magic.add(vk.SpacingDummyLayer(spacing=100))\n",
    "\n",
    "magic.add(Flatten(name='flat'))\n",
    "magic.add(Dense(1024, activation='relu', name='dense_1024'))\n",
    "magic.add(Dropout(0.5))\n",
    "magic.add(Dense(512, activation='relu', name='dense_512'))\n",
    "magic.add(Dropout(0.25))\n",
    "magic.add(Dense(128, activation='relu', name='dense_128'))\n",
    "magic.add(Dropout(0.1))\n",
    "magic.add(Dense(1))\n",
    "\n",
    "magic.load_weights('weights.h5', by_name=True,skip_mismatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic.compile(\n",
    "    optimizer=SGD(lr=1e-4, momentum=0.9, nesterov=True),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "print(magic.summary())\n",
    "vk.layered_view(magic, spacing=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyStopping(keras.callbacks.Callback):\n",
    "    def __init__(self, acc_threshold, verbose=1):\n",
    "        super(AccuracyStopping, self).__init__()\n",
    "        self._acc_threshold = acc_threshold\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        train_acc = logs.get('accuracy') != None and logs.get('accuracy')\n",
    "        if self.verbose > 0: print(f'Training Accuracy Threshold: {train_acc} / {self._acc_threshold}')\n",
    "        self.model.stop_training = train_acc >= self._acc_threshold\n",
    "acc_callback = AccuracyStopping(accuracy_threshold, verbose=0)\n",
    "pbar = TqdmCallback(verbose=1,tqdm_class=tqdm_notebook, leave = True, display = False)\n",
    "pbar.epoch_bar.ncols=0\n",
    "plot = PlotLossesKeras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pbar.display()\n",
    "history = magic.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=total_train // batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=0,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=total_val // batch_size,\n",
    "    callbacks=[pbar,plot,acc_callback]\n",
    ")"
   ]
  },
  {
   "source": [
    "![](cute_kitten.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic.save_weights('magic_weights.h5')"
   ]
  }
 ]
}